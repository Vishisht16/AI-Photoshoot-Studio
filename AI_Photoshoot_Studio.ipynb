{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "6U5B2MrDpI1j"
      ],
      "authorship_tag": "ABX9TyP45Ig8m9UFqcT6iFVO6+Ke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishisht16/AI-Photoshoot-Studio/blob/main/AI_Photoshoot_Studio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Photoshoot Studio"
      ],
      "metadata": {
        "id": "WvHwC-Zilylg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Please follow the instructions provided in this notebook to set up your own Studio through Colab and test out the code. No technical knowledge required!\n"
      ],
      "metadata": {
        "id": "vOUmu670nrJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Initial Cloud Hardware Setup\n",
        "\n",
        "> Go to the connect option at the top right, below your profile and click on the dropdown that will take you to 'Change runtime type'. Please go ahead and select 'T4 GPU' as your hardware accelerator and connect."
      ],
      "metadata": {
        "id": "0I6d8zm-oPja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Go to the cell below and press Ctrl/⌘ + Enter."
      ],
      "metadata": {
        "id": "FDNCg3cEqDs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq --upgrade numpy diffusers transformers accelerate mediapipe opencv-python-headless controlnet_aux torch gradio"
      ],
      "metadata": {
        "id": "RMPVEAyDTrPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the above step may take up to 2-3 minutes to be completed."
      ],
      "metadata": {
        "id": "5e4XGuHOrAdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2: Once the cell has finished running, click on 'Runtime' on the ribbon at the top and click 'Restart Session and run all'."
      ],
      "metadata": {
        "id": "x659I5ZNofHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The cells below contain the code required to create the whole application from beginning to end. You can check it out or choose to ignore the building process completely.\n",
        "\n"
      ],
      "metadata": {
        "id": "6U5B2MrDpI1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ot3aMEBlq6AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating folders for better abstraction\n",
        "!mkdir core app"
      ],
      "metadata": {
        "id": "QdJI7U_VJ9KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core/utils.py\n",
        "\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Initialize MediaPipe Face Detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
        "\n",
        "def detect_face(image: Image.Image) -> tuple | None:\n",
        "    \"\"\"\n",
        "    Detects the first face in a given PIL Image and returns its bounding box.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        np_image = np.array(image.convert(\"RGB\"))\n",
        "        results = face_detector.process(np_image)\n",
        "\n",
        "        if not results.detections:\n",
        "            print(\"Utils: No face detected.\")\n",
        "            return None\n",
        "\n",
        "        detection = results.detections[0]\n",
        "        bbox_data = detection.location_data.relative_bounding_box\n",
        "        img_h, img_w, _ = np_image.shape\n",
        "        x = int(bbox_data.xmin * img_w)\n",
        "        y = int(bbox_data.ymin * img_h)\n",
        "        w = int(bbox_data.width * img_w)\n",
        "        h = int(bbox_data.height * img_h)\n",
        "\n",
        "        print(f\"Utils: Face detected at [x={x}, y={y}, w={w}, h={h}]\")\n",
        "        return (x, y, w, h)\n",
        "    except Exception as e:\n",
        "        print(f\"Utils: An error occurred during face detection: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_inpainting_mask(image: Image.Image, bbox: tuple) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Creates a black and white mask for inpainting from an image and a bounding box.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        mask = Image.new(\"RGB\", image.size, \"black\")\n",
        "        draw = ImageDraw.Draw(mask)\n",
        "        x, y, w, h = bbox\n",
        "        shape = (x, y, x + w, y + h)\n",
        "        draw.rectangle(shape, fill=\"white\")\n",
        "        print(\"Utils: Inpainting mask created successfully.\")\n",
        "        return mask\n",
        "    except Exception as e:\n",
        "        print(f\"Utils: An error occurred during mask creation: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "o0rl_FkBZ_2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core/pipeline_manager.py\n",
        "\n",
        "import torch\n",
        "from diffusers import (\n",
        "    StableDiffusionPipeline, # <- ADDED: The standard pipeline\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    StableDiffusionControlNetInpaintPipeline,\n",
        "    ControlNetModel,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    DDPMScheduler,\n",
        ")\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from PIL import Image\n",
        "from core.utils import detect_face, create_inpainting_mask\n",
        "\n",
        "class SynthesisPipeline:\n",
        "    def __init__(self):\n",
        "        print(\"PipelineManager: Initializing...\")\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.torch_dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        print(f\"PipelineManager: Using device: {self.device} with dtype: {self.torch_dtype}\")\n",
        "\n",
        "        BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
        "        CONTROLNET_MODEL = \"lllyasviel/sd-controlnet-openpose\"\n",
        "\n",
        "        print(\"PipelineManager: Loading models...\")\n",
        "        # --- ControlNet specific models ---\n",
        "        self.openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
        "        self.controlnet = ControlNetModel.from_pretrained(\n",
        "            CONTROLNET_MODEL, torch_dtype=self.torch_dtype\n",
        "        )\n",
        "\n",
        "        # --- Load ALL THREE pipelines ---\n",
        "        # 1. Standard Text-to-Image Pipeline\n",
        "        self.base_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            BASE_MODEL, torch_dtype=self.torch_dtype, safety_checker=None\n",
        "        ).to(self.device)\n",
        "\n",
        "        # 2. ControlNet Pipeline\n",
        "        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            BASE_MODEL, controlnet=self.controlnet, torch_dtype=self.torch_dtype, safety_checker=None\n",
        "        ).to(self.device)\n",
        "\n",
        "        # 3. ControlNet Inpainting Pipeline\n",
        "        self.inpaint_pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
        "            BASE_MODEL, controlnet=self.controlnet, torch_dtype=self.torch_dtype, safety_checker=None\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.schedulers = {\n",
        "            \"PNDM\": PNDMScheduler.from_config(self.pipe.scheduler.config),\n",
        "            \"DDIM\": DDIMScheduler.from_config(self.pipe.scheduler.config),\n",
        "            \"DDPM\": DDPMScheduler.from_config(self.pipe.scheduler.config),\n",
        "        }\n",
        "        print(\"PipelineManager: All models loaded and ready!\")\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        control_image: Image.Image | None, # <- UPDATED: Now optional\n",
        "        preserve_face: bool,\n",
        "        scheduler_name: str,\n",
        "        num_steps: int = 25,\n",
        "        guidance_scale: float = 7.5,\n",
        "    ) -> tuple[Image.Image, Image.Image | None]: # <- UPDATED: Returns a tuple\n",
        "        print(\"\\n--- New Generation Request ---\")\n",
        "\n",
        "        scheduler = self.schedulers.get(scheduler_name, self.pipe.scheduler)\n",
        "        generator = torch.Generator(device=self.device).manual_seed(-1)\n",
        "\n",
        "        # THE NEW LOGIC: Check if a control image was provided\n",
        "        if control_image is not None:\n",
        "            # --- CONTROLNET MODE ---\n",
        "            print(\"Control image provided. Entering ControlNet Mode.\")\n",
        "\n",
        "            # Resize control image for consistency\n",
        "            control_image = control_image.resize((512, 512))\n",
        "            pose_image = self.openpose(control_image)\n",
        "\n",
        "            # Set scheduler for the relevant pipelines\n",
        "            self.pipe.scheduler = scheduler\n",
        "            self.inpaint_pipe.scheduler = scheduler\n",
        "\n",
        "            if preserve_face:\n",
        "                face_bbox = detect_face(control_image)\n",
        "                if face_bbox:\n",
        "                    mask_image = create_inpainting_mask(control_image, face_bbox)\n",
        "                    print(\"Using Inpainting Pipeline...\")\n",
        "                    result_image = self.inpaint_pipe(\n",
        "                        prompt=prompt, negative_prompt=negative_prompt, image=control_image,\n",
        "                        mask_image=mask_image, control_image=pose_image, num_inference_steps=num_steps,\n",
        "                        guidance_scale=guidance_scale, generator=generator\n",
        "                    ).images[0]\n",
        "                else: # Fallback if no face found\n",
        "                    result_image = self.pipe(\n",
        "                        prompt=prompt, negative_prompt=negative_prompt, image=pose_image,\n",
        "                        num_inference_steps=num_steps, guidance_scale=guidance_scale, generator=generator\n",
        "                    ).images[0]\n",
        "            else: # Standard ControlNet\n",
        "                result_image = self.pipe(\n",
        "                    prompt=prompt, negative_prompt=negative_prompt, image=pose_image,\n",
        "                    num_inference_steps=num_steps, guidance_scale=guidance_scale, generator=generator\n",
        "                ).images[0]\n",
        "\n",
        "            print(\"--- Generation Complete (ControlNet Mode) ---\")\n",
        "            return result_image, pose_image # Return both images\n",
        "\n",
        "        else:\n",
        "            # --- STANDARD TEXT-TO-IMAGE MODE ---\n",
        "            print(\"No control image provided. Entering Standard T2I Mode.\")\n",
        "\n",
        "            # Set scheduler for the base pipeline\n",
        "            self.base_pipe.scheduler = scheduler\n",
        "\n",
        "            result_image = self.base_pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_inference_steps=num_steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "\n",
        "            print(\"--- Generation Complete (Standard Mode) ---\")\n",
        "            return result_image, None"
      ],
      "metadata": {
        "id": "ly7r3bPvJCFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core/__init__.py"
      ],
      "metadata": {
        "id": "4xp-PO2Y9_1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app/__init__.py"
      ],
      "metadata": {
        "id": "OoC1Ljg9NAYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app/app_gradio.py\n",
        "\n",
        "import sys\n",
        "import gradio as gr\n",
        "\n",
        "sys.path.append('.')\n",
        "from core.pipeline_manager import SynthesisPipeline\n",
        "from app import ui_components\n",
        "\n",
        "print(\"App: Initializing the Synthesis Pipeline...\")\n",
        "try:\n",
        "    synthesis_studio = SynthesisPipeline()\n",
        "    print(\"App: Synthesis Pipeline initialized successfully.\")\n",
        "except Exception:\n",
        "    import traceback\n",
        "    print(\"FATAL: Failed to initialize SynthesisPipeline.\")\n",
        "    traceback.print_exc()\n",
        "    synthesis_studio = None\n",
        "\n",
        "# UPDATED: The logic for handling 'None' is now in the backend. This function is simpler.\n",
        "def run_inference(pos_prompt, neg_prompt, control_image, preserve_face, scheduler, steps, guidance, progress=gr.Progress(track_tqdm=True)):\n",
        "    if synthesis_studio is None:\n",
        "        raise gr.Error(\"Models could not be loaded. Application is not functional.\")\n",
        "\n",
        "    # REMOVED: The check for control_image is GONE!\n",
        "\n",
        "    progress(0.1, desc=\"Starting generation...\")\n",
        "\n",
        "    # The backend now returns a tuple (generated_image, pose_image)\n",
        "    # pose_image will be None if no control_image was provided.\n",
        "    generated_image, pose_image = synthesis_studio.generate_image(\n",
        "        prompt=pos_prompt, negative_prompt=neg_prompt,\n",
        "        control_image=control_image, preserve_face=preserve_face,\n",
        "        scheduler_name=scheduler, num_steps=int(steps), guidance_scale=float(guidance),\n",
        "    )\n",
        "\n",
        "    progress(1.0, desc=\"Done!\")\n",
        "    return generated_image, pose_image\n",
        "\n",
        "def build_app():\n",
        "    with gr.Blocks(css=ui_components.CUSTOM_CSS, theme=gr.themes.Soft(primary_hue=\"orange\", secondary_hue=\"yellow\")) as interface:\n",
        "        if synthesis_studio is None:\n",
        "            gr.Markdown(\"# ❌ Application Error\\nThe AI models failed to load. Please check console logs for details.\")\n",
        "            return interface\n",
        "\n",
        "        scheduler_list = list(synthesis_studio.schedulers.keys())\n",
        "        c = ui_components.create_ui_layout(scheduler_list)\n",
        "\n",
        "        def autofill_negative_prompt(is_checked, current_text):\n",
        "            common_negs = ui_components.COMMON_NEGATIVE_PROMPTS\n",
        "            if is_checked:\n",
        "                return f\"{current_text}, {common_negs}\" if common_negs not in current_text else current_text\n",
        "            else:\n",
        "                return current_text.replace(f\", {common_negs}\", \"\").replace(common_negs, \"\").strip().rstrip(',')\n",
        "\n",
        "        def clear_negative_prompt():\n",
        "            return \"\", False\n",
        "\n",
        "        def handle_image_upload(image):\n",
        "            return gr.update(visible=image is not None)\n",
        "\n",
        "        def reset_settings():\n",
        "            return scheduler_list[0], 25, 7.5\n",
        "\n",
        "        c[\"autofill_neg_checkbox\"].change(fn=autofill_negative_prompt, inputs=[c[\"autofill_neg_checkbox\"], c[\"negative_prompt\"]], outputs=c[\"negative_prompt\"])\n",
        "        c[\"clear_neg_button\"].click(fn=clear_negative_prompt, inputs=None, outputs=[c[\"negative_prompt\"], c[\"autofill_neg_checkbox\"]])\n",
        "        c[\"control_image_input\"].upload(fn=handle_image_upload, inputs=c[\"control_image_input\"], outputs=c[\"preserve_face_checkbox\"])\n",
        "        c[\"control_image_input\"].clear(fn=lambda: gr.update(visible=False), inputs=None, outputs=c[\"preserve_face_checkbox\"])\n",
        "        c[\"reset_button\"].click(fn=reset_settings, inputs=None, outputs=[c[\"scheduler_dropdown\"], c[\"steps_slider\"], c[\"guidance_slider\"]])\n",
        "        c[\"generate_button\"].click(\n",
        "            fn=run_inference,\n",
        "            inputs=[c[\"positive_prompt\"], c[\"negative_prompt\"], c[\"control_image_input\"], c[\"preserve_face_checkbox\"], c[\"scheduler_dropdown\"], c[\"steps_slider\"], c[\"guidance_slider\"]],\n",
        "            outputs=[c[\"output_image\"], c[\"pose_image\"]]\n",
        "        )\n",
        "\n",
        "    return interface\n",
        "\n",
        "app_instance = build_app()"
      ],
      "metadata": {
        "id": "N_sjFZHe87X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app/ui_components.py\n",
        "\n",
        "\n",
        "# Disclaimer: The code in this cell was AI-generated.\n",
        "import gradio as gr\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    \"\"\"Image ko read karke use Base64 string mein convert karta hai.\"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        return f\"data:image/png;base64,{encoded_string}\"\n",
        "    except FileNotFoundError:\n",
        "        print(f\"WARNING: Background image not found at {image_path}. Using a plain background.\")\n",
        "        return \"none\"\n",
        "\n",
        "background_image_path = Path(\"background.png\")\n",
        "base64_background = encode_image_to_base64(background_image_path)\n",
        "\n",
        "COMMON_NEGATIVE_PROMPTS = \"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face, blurry, draft, grainy\"\n",
        "\n",
        "# Your custom CSS remains untouched\n",
        "CUSTOM_CSS = f\"\"\"\n",
        "@import url('https://fonts.googleapis.com/css2?family=Josefin+Sans:wght@400;600;700&display=swap');\n",
        "body {{ background-image: url('{base64_background}') !important; background-size: cover !important; background-position: center !important; background-attachment: fixed !important; }}\n",
        ".gradio-container {{ background: none !important; }}\n",
        "* {{ font-family: 'Josefin Sans', sans-serif !important; color: #EAEAEA !important; }}\n",
        ".gradio-group, .gradio-tabs, .gradio-accordion, .gradio-html {{ background-color: rgba(20, 20, 30, 0.4) !important; backdrop-filter: blur(12px) !important; -webkit-backdrop-filter: blur(12px) !important; border: 1px solid rgba(255, 255, 255, 0.1) !important; border-radius: 12px !important; box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1) !important; }}\n",
        "#main_header {{ background: none !important; border: none !important; box-shadow: none !important; }}\n",
        "textarea, input[type=\"text\"], input[type=\"number\"] {{ background-color: rgba(0, 0, 10, 0.5) !important; border: 1px solid rgba(255, 255, 255, 0.2) !important; }}\n",
        "#main_header h1 {{ font-weight: 700; font-size: 2.8em; color: #FFFFFF !important; text-shadow: 0px 2px 4px rgba(0,0,0,0.4); }}\n",
        "#main_header p {{ font-size: 1.2em; color: #B0C4DE !important; }}\n",
        "h3 {{ font-weight: 600 !important; color: #FFFFFF !important; text-transform: uppercase !important; letter-spacing: 1px !important; border-bottom: 1px solid rgba(255, 255, 255, 0.2); padding-bottom: 5px; }}\n",
        "#github_icon {{ display: flex; justify-content: flex-end; align-items: center; height: 100%; background: none !important; }}\n",
        "#github_icon a {{ color: #EAEAEA !important; font-size: 1.8em; transition: color 0.3s, transform 0.3s; }}\n",
        "#github_icon a:hover {{ color: #FFFFFF !important; transform: scale(1.1); }}\n",
        "\"\"\"\n",
        "\n",
        "# <<< CHANGE 1: UPDATED GUIDE MARKDOWN >>>\n",
        "# The guide now explains both Standard and ControlNet modes.\n",
        "GUIDE_MARKDOWN = \"\"\"\n",
        "## How to Use This Studio 📖\n",
        "\n",
        "This studio can generate images in two ways:\n",
        "1.  **Standard Mode:** Simply write a prompt and click Generate.\n",
        "2.  **ControlNet Mode:** Upload a \"Control Image\" to guide the structure of the generated image.\n",
        "\n",
        "#### **What is a \"Control Image\"? (Optional Feature)**\n",
        "The \"Control Image\" is your blueprint. The AI doesn't copy the *style* of this image, but it copies the **structure** (like a person's pose).\n",
        "\n",
        "-   **What it does:** The final generated image will have a person in the *exact same pose* as the person in your control image.\n",
        "-   **When to use it:** Use this when you want to control the composition or pose of a person in your image.\n",
        "\n",
        "#### **How to Use \"Attempt to preserve face?\"**\n",
        "This feature **only works if you have uploaded a Control Image**.\n",
        "\n",
        "-   **Result:** It will try to make the face in the generated image look like the face from your uploaded image.\n",
        "\n",
        "#### **3. Example Prompts**\n",
        "-   **Standard Prompt:** `A majestic lion in the savannah, golden hour, photorealistic, 8k`\n",
        "-   **ControlNet Prompt:** `A photorealistic portrait of a royal queen in a magnificent golden dress, cinematic lighting, highly detailed`\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "ADVANCED_GUIDE_MARKDOWN = \"\"\"\n",
        "### **Advanced Settings Explained**\n",
        "-   **Scheduler:** This is the algorithm used for denoising the image. Different schedulers can produce slightly different styles and textures.\n",
        "    -   **`DDIM` / `PNDM`:** Good all-rounders, fast and reliable.\n",
        "    -   **`DDPM`:** Often produces high-quality results but can be slower.\n",
        "-   **Inference Steps:** How many steps the AI takes to generate the image.\n",
        "    -   **More steps (e.g., 40-50):** Can add more detail but takes longer.\n",
        "    -   **Fewer steps (e.g., 20-25):** Faster generation, great for testing ideas.\n",
        "-   **Guidance Scale (CFG):** How strictly the AI should follow your positive prompt.\n",
        "    -   **Higher value (e.g., 10-15):** The AI will stick very closely to your prompt, but might be less creative.\n",
        "    -   **Lower value (e.g., 7-9):** A good balance of creativity and prompt adherence.\n",
        "\"\"\"\n",
        "\n",
        "def create_ui_layout(scheduler_choices):\n",
        "    \"\"\"\n",
        "    Creates and returns a dictionary of all the Gradio UI components.\n",
        "    \"\"\"\n",
        "    with gr.Row(elem_id=\"main_header\"):\n",
        "        with gr.Column(scale=10):\n",
        "            gr.HTML(\"<h1>AI Photoshoot Studio 🎨</h1><p>Turn your ideas into stunning visuals with precise control.</p>\")\n",
        "        with gr.Column(scale=1, min_width=50, elem_id=\"github_icon\"):\n",
        "            gr.HTML('''<a href=\"https://github.com/Vishisht16/AI-Photoshoot-Studio\" target=\"_blank\" title=\"View on GitHub\" style=\"display: inline-block; padding: 8px; background-color: #f0f0f0; border-radius: 6px; text-decoration: none;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"32\" height=\"32\" viewBox=\"0 0 24 24\"><path fill=\"#000000\" d=\"M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.09.68-.22.68-.48v-1.7c-2.78.6-3.37-1.34-3.37-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.89 1.53 2.34 1.09 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.95c0-1.1.39-1.99 1.03-2.69a3.6 3.6 0 0 1 .1-2.64s.84-.27 2.75 1.02a9.58 9.58 0 0 1 5 0c1.91-1.29 2.75-1.02 2.75-1.02c.53 1.28.018 2.37.1 2.64c.64.7 1.03 1.6 1.03 2.69c0 3.85-2.34 4.7-4.57 4.94c.36.31.68.92.68 1.85v2.72c0 .27.18.58.69.48A10 10 0 0 0 22 12A10 10 0 0 0 12 2\"/></svg></a>''')\n",
        "\n",
        "    with gr.Row(equal_height=False):\n",
        "        with gr.Column(scale=1):\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### 1. Describe Your Image\")\n",
        "                # <<< CHANGE 2: UPDATED PLACEHOLDER TEXT >>>\n",
        "                positive_prompt = gr.Textbox(label=\"Positive Prompt\", placeholder=\"e.g., A majestic lion in the savannah, golden hour...\", lines=3)\n",
        "                negative_prompt = gr.Textbox(label=\"Negative Prompt\", lines=2, value=COMMON_NEGATIVE_PROMPTS)\n",
        "                with gr.Row():\n",
        "                    autofill_neg_checkbox = gr.Checkbox(label=\"Auto-fill negatives\", value=True)\n",
        "                    clear_neg_button = gr.Button(\"Clear\")\n",
        "\n",
        "            with gr.Group():\n",
        "                # <<< CHANGE 3: HEADING UPDATED TO \"OPTIONAL\" >>>\n",
        "                gr.Markdown(\"### 2. (Optional) Provide a Control Image\")\n",
        "                control_image_input = gr.Image(label=\"Upload Image to Control Pose\", type=\"pil\", image_mode=\"RGB\", height=300)\n",
        "                preserve_face_checkbox = gr.Checkbox(label=\"Attempt to preserve face?\", value=False, visible=False)\n",
        "\n",
        "            with gr.Group():\n",
        "                gr.Markdown(\"### 3. Adjust Settings\")\n",
        "                with gr.Accordion(\"Advanced Settings\", open=True):\n",
        "                    scheduler_dropdown = gr.Dropdown(label=\"Scheduler\", choices=scheduler_choices, value=scheduler_choices[0])\n",
        "                    steps_slider = gr.Slider(label=\"Inference Steps\", minimum=10, maximum=100, step=1, value=25)\n",
        "                    guidance_slider = gr.Slider(label=\"Guidance Scale (CFG)\", minimum=1.0, maximum=20.0, step=0.5, value=7.5)\n",
        "                    reset_button = gr.Button(\"Reset to Defaults\")\n",
        "\n",
        "            generate_button = gr.Button(\"Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"Generated Image\"):\n",
        "                    output_image = gr.Image(label=\"Final Output\", height=440)\n",
        "                with gr.TabItem(\"Detected Pose\"):\n",
        "                    # <<< CHANGE 4: LABEL UPDATED FOR CLARITY >>>\n",
        "                    pose_image = gr.Image(label=\"Detected Pose (Only if Control Image is used)\", height=440)\n",
        "\n",
        "            with gr.Accordion(\"Click me to toggle User Manual!\", open=False):\n",
        "                gr.Markdown(GUIDE_MARKDOWN)\n",
        "                with gr.Accordion(\"Click me to know more about Advanced Settings!\", open=False):\n",
        "                    gr.Markdown(ADVANCED_GUIDE_MARKDOWN)\n",
        "\n",
        "    return { \"positive_prompt\": positive_prompt, \"negative_prompt\": negative_prompt, \"autofill_neg_checkbox\": autofill_neg_checkbox, \"clear_neg_button\": clear_neg_button, \"control_image_input\": control_image_input, \"preserve_face_checkbox\": preserve_face_checkbox, \"scheduler_dropdown\": scheduler_dropdown, \"steps_slider\": steps_slider, \"guidance_slider\": guidance_slider, \"generate_button\": generate_button, \"reset_button\": reset_button, \"output_image\": output_image, \"pose_image\": pose_image }"
      ],
      "metadata": {
        "id": "CW_tUCkM-qvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from app.app_gradio import app_instance\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=======================================\")\n",
        "    print(\"🚀 Launching AI Photoshoot Studio...\")\n",
        "    print(\"=======================================\")\n",
        "    app_instance.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        share=True,\n",
        "        debug=True\n",
        "    )"
      ],
      "metadata": {
        "id": "3v5CBNoM-6ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w-uKA3F4pckk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The following cell creates a public URL within 3-4 minutes, that you can browse to access and test the AI Photoshoot Studio for as long as the Colab notebook is running."
      ],
      "metadata": {
        "id": "yU37rYrTrbeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "id": "p5HpwCj9_uMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once finished, please go to 'Runtime' and click on 'Disconnect and delete runtime' to save valuable resources.\n",
        "Thank you for the visit!"
      ],
      "metadata": {
        "id": "v4OwnZyosLnK"
      }
    }
  ]
}